import os
import argparse
import jinja2
<<<<<<< HEAD
import time
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from langchain_community.llms import HuggingFacePipeline
from dotenv import load_dotenv
import sys
from utils import util as u
import torch
from langchain_community.embeddings import SentenceTransformerEmbeddings

=======
import asyncio
from dotenv import load_dotenv
from utils import util as u
>>>>>>> 37c4389 (ë°°í¬ ì „ ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ)

load_dotenv()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1) ê²½ë¡œÂ·í™˜ê²½ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_PATH  = os.path.dirname(os.path.abspath(__file__))
PROMPT_DIR = os.path.join(BASE_PATH, "prompts")
<<<<<<< HEAD

# â†“â†“â†“  db_test/<ì—¬ê¸°ì— db ì…ë ¥>  ë¶€ë¶„ë§Œ ë°”ê¿”ì£¼ì„¸ìš” â†“â†“â†“
DB_SUBDIR = "LAW_RAG_500_75"
#DB_DIR    = os.path.join(BASE_PATH, "db_test", DB_SUBDIR)
# â†‘â†‘â†‘  db_test/<ì—¬ê¸°ì— db ì…ë ¥>  ë¶€ë¶„ë§Œ ë°”ê¿”ì£¼ì„¸ìš” â†‘â†‘â†‘
=======
DB_SUBDIR = "LAW_RAG_500_75"
>>>>>>> 37c4389 (ë°°í¬ ì „ ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ)

OPENAI_KEY = os.getenv("OPENAI_API_KEY")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2) DB ì ‘ì† ì •ë³´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
<<<<<<< HEAD
# (create_db.retrieve_db í˜¸ì¶œì„ ìœ„í•´ í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •)
=======
>>>>>>> 37c4389 (ë°°í¬ ì „ ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ)
HOST        = os.getenv("DB_HOST")
PORT        = int(os.getenv("DB_PORT", 3306))
USER        = os.getenv("DB_USER")
PASSWORD    = os.getenv("DB_PASSWORD")
DB_NAME     = os.getenv("DB_NAME")
<<<<<<< HEAD
#BASE_DB_DIR = os.path.join(BASE_PATH, "db_test")   # create_db.pyì˜ base_db_dir ì¸ì
=======
>>>>>>> 37c4389 (ë°°í¬ ì „ ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3) Jinja2 í…œí”Œë¦¿ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
env = jinja2.Environment(
    loader=jinja2.FileSystemLoader(PROMPT_DIR),
    autoescape=False,
    trim_blocks=True,
    lstrip_blocks=True,
)
<<<<<<< HEAD
#query_tpl  = env.get_template("query_rewrite.j2")  # í…œí”Œë¦¿ íŒŒì¼ëª…ì´ ì •í™•í•œì§€ í™•ì¸í•˜ì„¸ìš” :contentReference[oaicite:0]{index=0}
answer_tpl = env.get_template("answer_synth.j2")

def get_llm():
    return ChatOpenAI(api_key=OPENAI_KEY, model="gpt-4o-mini", temperature=1)

def rewrite_query(user_query: str) -> str:
    prompt = query_tpl.render(user_query=user_query)
    llm = get_llm()
    print("ğŸ”„ ì§ˆì˜ ì¬ì‘ì„±(LLM) â€¦", end=" ")
    start = time.perf_counter()
    resp = llm.invoke(prompt)
    print(f"âœ” ({time.perf_counter()-start:.1f}s)")
    return resp.content.strip() if hasattr(resp, "content") else resp.strip()

def run_rag(user_query: str, vectorstore, k: int = 5) -> str:
    # 1) ê²€ìƒ‰ì–´ ì¬ì‘ì„±
    #search_query = rewrite_query(user_query)
    #print("   â†ª ê²€ìƒ‰ì–´:", search_query)

    # 2) MySQL+Chroma í†µí•© ì „ë¬¸ ì¡°íšŒ (create_db.retrieve_db í˜¸ì¶œ)
    print("DB ê²€ìƒ‰ ì¤‘...")
    results = u.retrieve_db(
        user_query,
        HOST, PORT, USER, PASSWORD, DB_NAME,vectorstore,k=k
    )

    # 3) ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ ë° í…œí”Œë¦¿ ì ìš©
    if not results or len(results) == 0:
        return "ìœ ì‚¬ íŒë¡€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    # ìƒìœ„ kê°œ ê²°ê³¼ ëª¨ë‘ ë³‘í•©
    contexts = []
    full_documents = []

    for i, item in enumerate(results):
        # ê° ê²°ê³¼ì—ì„œ í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ
        contexts.append(f"{i+1}. {item['ìœ ì‚¬ë¬¸ë‹¨']} [íŒë¡€ë²ˆí˜¸:{item['íŒë¡€ì¼ë ¨ë²ˆí˜¸']}]")
        full_documents.append(f"--- ë¬¸ì„œ {i+1} ---\n{item['ì „ë¬¸']}")

    answer = answer_tpl.render(
        context1=results[0]['ìœ ì‚¬ë¬¸ë‹¨'] + f" [íŒë¡€ë²ˆí˜¸:{results[0]['íŒë¡€ì¼ë ¨ë²ˆí˜¸']}]",
        full1=results[0]['ì „ë¬¸'],
        context2=results[1]['ìœ ì‚¬ë¬¸ë‹¨'] + f" [íŒë¡€ë²ˆí˜¸:{results[1]['íŒë¡€ì¼ë ¨ë²ˆí˜¸']}]",
        full2=results[1]['ì „ë¬¸'],
        context3=results[2]['ìœ ì‚¬ë¬¸ë‹¨'] + f" [íŒë¡€ë²ˆí˜¸:{results[2]['íŒë¡€ì¼ë ¨ë²ˆí˜¸']}]",
        full3=results[2]['ì „ë¬¸'],
        context4=results[3]['ìœ ì‚¬ë¬¸ë‹¨'] + f" [íŒë¡€ë²ˆí˜¸:{results[3]['íŒë¡€ì¼ë ¨ë²ˆí˜¸']}]",
        full4=results[3]['ì „ë¬¸'],
        context5=results[4]['ìœ ì‚¬ë¬¸ë‹¨'] + f" [íŒë¡€ë²ˆí˜¸:{results[4]['íŒë¡€ì¼ë ¨ë²ˆí˜¸']}]",
        full5=results[4]['ì „ë¬¸'],
        user_query=user_query
    )

    
    llm = get_llm()
    print("ğŸ”„ ë‹µë³€ ìƒì„±(LLM) â€¦", end=" ")
    start = time.perf_counter()
    resp = llm.invoke(answer)
    print(f"âœ” ({time.perf_counter()-start:.1f}s)")
    
    return resp.content.strip() if hasattr(resp, "content") else resp.strip()

def setup_db(base_db_dir='./db'):
    cuda_available = torch.cuda.is_available()
    if cuda_available:
        print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
        print(f"   GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    else:
        print("âŒ CUDA ì‚¬ìš© ë¶ˆê°€ëŠ¥ - CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤")
    device = "cuda" if cuda_available else "cpu"
    
    vectorstore = Chroma(
        persist_directory=base_db_dir,
        embedding_function=SentenceTransformerEmbeddings(model_name='nlpai-lab/KURE-v1', model_kwargs={"device": device}),
        collection_name='LAW_RAG_500_75'
    )
    print('ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ')
    return vectorstore

def main():
    vectorstore = setup_db()
    while True:
        user_query = input("ğŸ’¬ ì²˜í•œ ë²•ì  ìƒí™©ê³¼ ê±±ì •í•˜ëŠ” ì ì„ ì•Œë ¤ì£¼ì„¸ìš”: ").strip()
        if user_query == 'exit':
            break
        answer = run_rag(user_query,vectorstore)
        print(user_query)
        print("ğŸ“Œ ìµœì¢… ìš”ì•½\n", answer)
        
    
    return answer

if __name__ == "__main__":
    main()
=======

answer_tpl = env.get_template("answer_synth.j2")
conversation_tpl = env.get_template("conversation.j2")

async def main():
    vectorstore = u.setup_db()
    conn = u.get_mysql_connection(HOST,PORT,USER,PASSWORD,DB_NAME)
    while True:
        user_query = input("ğŸ’¬ ì²˜í•œ ë²•ì  ìƒí™©ê³¼ ê±±ì •í•˜ëŠ” ì ì„ ì•Œë ¤ì£¼ì„¸ìš”: ").strip()
        if not user_query:
            continue
            
        print(f"\nì§ˆë¬¸: {user_query}")
        
        # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë‹µë³€ ìƒì„±
        answer = ''
        async for chunk in u.run_rag_stream(user_query, vectorstore, 5, conn, answer_tpl, OPENAI_KEY):
            print(chunk, end="", flush=True)
            answer += chunk
        
        print("\n")  # ë§ˆì§€ë§‰ ì¤„ë°”ê¿ˆ
        
        # ë‹µë³€ì´ ìœ íš¨í•œì§€ í™•ì¸
        if 'ì¡°ê¸ˆ ë” ìì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”' not in answer and 'ê´€ë ¨ íŒë¡€ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤' not in answer:
            break
    
    '''while True:
        user_query = input("í•´ë‹¹ ë‚´ìš©ì— ëŒ€í•´ ì¶”ê°€ì ìœ¼ë¡œ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹ ê°€ìš”? ").strip()
        if user_query == 'exit':
            break
        answer = u.run_rag(user_query,vectorstore,5,HOST,PORT,USER,PASSWORD,DB_NAME)
        print(user_query)
        print("ğŸ“Œ ë‹µë³€ \n", answer)'''
    
    #return answer
    model = u.get_llm(OPENAI_KEY)
    conversation = u.set_conversation(user_query,answer,model)
    while True:
        user_query = input("í•´ë‹¹ ë‚´ìš©ì— ëŒ€í•´ ì¶”ê°€ì ìœ¼ë¡œ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹ ê°€ìš”? ").strip()
        if user_query == 'exit':
            break
        async for chunk in u.run_conversation(conversation,user_query,vectorstore,conn,3,conversation_tpl,OPENAI_KEY):
            print(chunk, end="", flush=True)
        print("\n")
        

if __name__ == "__main__":
    asyncio.run(main())
>>>>>>> 37c4389 (ë°°í¬ ì „ ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ)
