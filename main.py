import os
import argparse
import jinja2
import time
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from langchain_community.llms import HuggingFacePipeline
from dotenv import load_dotenv
import sys
from utils import util as u
import torch
from langchain_community.embeddings import SentenceTransformerEmbeddings


load_dotenv()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1) ê²½ë¡œÂ·í™˜ê²½ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_PATH  = os.path.dirname(os.path.abspath(__file__))
PROMPT_DIR = os.path.join(BASE_PATH, "prompts")

# â†“â†“â†“  db_test/<ì—¬ê¸°ì— db ì…ë ¥>  ë¶€ë¶„ë§Œ ë°”ê¿”ì£¼ì„¸ìš” â†“â†“â†“
DB_SUBDIR = "LAW_RAG_500_75"
#DB_DIR    = os.path.join(BASE_PATH, "db_test", DB_SUBDIR)
# â†‘â†‘â†‘  db_test/<ì—¬ê¸°ì— db ì…ë ¥>  ë¶€ë¶„ë§Œ ë°”ê¿”ì£¼ì„¸ìš” â†‘â†‘â†‘

OPENAI_KEY = os.getenv("OPENAI_API_KEY")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2) DB ì ‘ì† ì •ë³´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# (create_db.retrieve_db í˜¸ì¶œì„ ìœ„í•´ í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •)
HOST        = os.getenv("DB_HOST")
PORT        = int(os.getenv("DB_PORT", 3306))
USER        = os.getenv("DB_USER")
PASSWORD    = os.getenv("DB_PASSWORD")
DB_NAME     = os.getenv("DB_NAME")
#BASE_DB_DIR = os.path.join(BASE_PATH, "db_test")   # create_db.pyì˜ base_db_dir ì¸ì

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3) Jinja2 í…œí”Œë¦¿ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
env = jinja2.Environment(
    loader=jinja2.FileSystemLoader(PROMPT_DIR),
    autoescape=False,
    trim_blocks=True,
    lstrip_blocks=True,
)
#query_tpl  = env.get_template("query_rewrite.j2")  # í…œí”Œë¦¿ íŒŒì¼ëª…ì´ ì •í™•í•œì§€ í™•ì¸í•˜ì„¸ìš” :contentReference[oaicite:0]{index=0}
answer_tpl = env.get_template("answer_synth.j2")

def get_llm():
    return ChatOpenAI(api_key=OPENAI_KEY, model="gpt-4o-mini", temperature=1)

def rewrite_query(user_query: str) -> str:
    prompt = query_tpl.render(user_query=user_query)
    llm = get_llm()
    print("ğŸ”„ ì§ˆì˜ ì¬ì‘ì„±(LLM) â€¦", end=" ")
    start = time.perf_counter()
    resp = llm.invoke(prompt)
    print(f"âœ” ({time.perf_counter()-start:.1f}s)")
    return resp.content.strip() if hasattr(resp, "content") else resp.strip()

def run_rag(user_query: str, vectorstore, k: int = 5) -> str:
    # 1) ê²€ìƒ‰ì–´ ì¬ì‘ì„±
    #search_query = rewrite_query(user_query)
    #print("   â†ª ê²€ìƒ‰ì–´:", search_query)

    # 2) MySQL+Chroma í†µí•© ì „ë¬¸ ì¡°íšŒ (create_db.retrieve_db í˜¸ì¶œ)
    print("DB ê²€ìƒ‰ ì¤‘...")
    results = u.retrieve_db(
        user_query,
        HOST, PORT, USER, PASSWORD, DB_NAME,vectorstore,k=k
    )

    # 3) ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ ë° í…œí”Œë¦¿ ì ìš©
    if not results or len(results) == 0:
        return "ìœ ì‚¬ íŒë¡€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    # ìƒìœ„ kê°œ ê²°ê³¼ ëª¨ë‘ ë³‘í•©
    contexts = []
    full_documents = []

    for i, item in enumerate(results):
        # ê° ê²°ê³¼ì—ì„œ í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ
        contexts.append(f"{i+1}. {item['ìœ ì‚¬ë¬¸ë‹¨']} [íŒë¡€ë²ˆí˜¸:{item['íŒë¡€ì¼ë ¨ë²ˆí˜¸']}]")
        full_documents.append(f"--- ë¬¸ì„œ {i+1} ---\n{item['ì „ë¬¸']}")

    answer = answer_tpl.render(
        context1=results[0]['ìœ ì‚¬ë¬¸ë‹¨'] + f" [íŒë¡€ë²ˆí˜¸:{results[0]['íŒë¡€ì¼ë ¨ë²ˆí˜¸']}]",
        full1=results[0]['ì „ë¬¸'],
        context2=results[1]['ìœ ì‚¬ë¬¸ë‹¨'] + f" [íŒë¡€ë²ˆí˜¸:{results[1]['íŒë¡€ì¼ë ¨ë²ˆí˜¸']}]",
        full2=results[1]['ì „ë¬¸'],
        context3=results[2]['ìœ ì‚¬ë¬¸ë‹¨'] + f" [íŒë¡€ë²ˆí˜¸:{results[2]['íŒë¡€ì¼ë ¨ë²ˆí˜¸']}]",
        full3=results[2]['ì „ë¬¸'],
        context4=results[3]['ìœ ì‚¬ë¬¸ë‹¨'] + f" [íŒë¡€ë²ˆí˜¸:{results[3]['íŒë¡€ì¼ë ¨ë²ˆí˜¸']}]",
        full4=results[3]['ì „ë¬¸'],
        context5=results[4]['ìœ ì‚¬ë¬¸ë‹¨'] + f" [íŒë¡€ë²ˆí˜¸:{results[4]['íŒë¡€ì¼ë ¨ë²ˆí˜¸']}]",
        full5=results[4]['ì „ë¬¸'],
        user_query=user_query
    )

    
    llm = get_llm()
    print("ğŸ”„ ë‹µë³€ ìƒì„±(LLM) â€¦", end=" ")
    start = time.perf_counter()
    resp = llm.invoke(answer)
    print(f"âœ” ({time.perf_counter()-start:.1f}s)")
    
    return resp.content.strip() if hasattr(resp, "content") else resp.strip()

def setup_db(base_db_dir='./db'):
    cuda_available = torch.cuda.is_available()
    if cuda_available:
        print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
        print(f"   GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    else:
        print("âŒ CUDA ì‚¬ìš© ë¶ˆê°€ëŠ¥ - CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤")
    device = "cuda" if cuda_available else "cpu"
    
    vectorstore = Chroma(
        persist_directory=base_db_dir,
        embedding_function=SentenceTransformerEmbeddings(model_name='nlpai-lab/KURE-v1', model_kwargs={"device": device}),
        collection_name='LAW_RAG_500_75'
    )
    print('ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ')
    return vectorstore

def main():
    vectorstore = setup_db()
    while True:
        user_query = input("ğŸ’¬ ì²˜í•œ ë²•ì  ìƒí™©ê³¼ ê±±ì •í•˜ëŠ” ì ì„ ì•Œë ¤ì£¼ì„¸ìš”: ").strip()
        if user_query == 'exit':
            break
        answer = run_rag(user_query,vectorstore)
        print(user_query)
        print("ğŸ“Œ ìµœì¢… ìš”ì•½\n", answer)
        
    
    return answer

if __name__ == "__main__":
    main()