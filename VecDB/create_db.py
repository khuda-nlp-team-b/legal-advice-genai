from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings   
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.schema import Document
import pandas as pd
import pymysql
import os
import warnings
import chromadb
from langchain.embeddings import SentenceTransformerEmbeddings
<<<<<<< HEAD
=======
import torch
import time
from datetime import datetime, timedelta
>>>>>>> c999bd342716fb314c0a6ed0f194555535ac501e
warnings.filterwarnings('ignore')


def get_mysql_connection(host,port,username,password,db_name):
    try:
        conn = pymysql.connect(host=host, port=port, user=username, password=password, db=db_name,charset='utf8',cursorclass=pymysql.cursors.DictCursor)
        print(f"MySQL ì—°ê²° ì„±ê³µ")
        return conn
    except Exception as e:
        print(f"MySQL ì—°ê²° ì˜¤ë¥˜: {e}")
        return None

def get_document(conn,source):
    try: 
        with conn.cursor() as cursor:
            sql = f'SELECT íŒë¡€ì¼ë ¨ë²ˆí˜¸, íŒë¡€ë‚´ìš© FROM íŒë¡€ WHERE íŒë¡€ì¼ë ¨ë²ˆí˜¸ = {source}'
            cursor.execute(sql)
            result = cursor.fetchone()
            return result
    except Exception as e:
        print(f"MySQL ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return None



def save_df(host,port,username,password,db_name):
    conn = pymysql.connect(host=host, port=port, user=username, password=password, db=db_name)
    df = pd.read_sql('SELECT * FROM íŒë¡€', conn)
    df.to_csv('íŒë¡€.csv', index=False)
    conn.close()
    

def create_db(base_db_dir='./db'):
    # CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    cuda_available = torch.cuda.is_available()
    if cuda_available:
        print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
        print(f"   GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    else:
        print("âŒ CUDA ì‚¬ìš© ë¶ˆê°€ëŠ¥ - CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤")
    
    # ì‹œì‘ ì‹œê°„ ê¸°ë¡
    start_time = time.time()
    
    # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ íŒŒì¼ ê²½ë¡œ ì„¤ì •
    script_dir = os.path.dirname(os.path.abspath(__file__))
    csv_file_path = os.path.join(script_dir, 'íŒë¡€.csv')
    
    # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if not os.path.exists(csv_file_path):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_file_path}")
        print(f"í˜„ì¬ ë””ë ‰í† ë¦¬: {os.getcwd()}")
        print(f"ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬: {script_dir}")
        print("ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì¼ë“¤:")
        for file in os.listdir(script_dir):
            if file.endswith('.csv'):
                print(f"  - {file}")
        return
    
    # íŒë¡€.csv íŒŒì¼ ì½ê¸°
    print(f"ğŸ“ íŒŒì¼ ê²½ë¡œ: {csv_file_path}")
    df_íŒë¡€ = pd.read_csv(csv_file_path)
    print(f"ğŸ“Š ì´ {len(df_íŒë¡€)}ê°œì˜ íŒë¡€ ë°ì´í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤")

    # ê¸°ì¡´ ë²¡í„° DB ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ë©´ ì™„ì „ ì‚­ì œ
    if os.path.exists(base_db_dir):
        try:
            import shutil
            shutil.rmtree(base_db_dir)
            print(f"[ì‚­ì œ] ê¸°ì¡´ ë²¡í„° DB ë””ë ‰í† ë¦¬ê°€ ì™„ì „íˆ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤: {base_db_dir}")
        except Exception as e:
            print(f"[ê²½ê³ ] ê¸°ì¡´ DB ë””ë ‰í† ë¦¬ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    # DB ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(base_db_dir, exist_ok=True)
    
    # Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    print('Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ ìƒì„± ì¤‘...')
    docs = []
    for i, row in df_íŒë¡€.iterrows():
        metadata = {
            "source": row['íŒë¡€ì¼ë ¨ë²ˆí˜¸'],
            'case_type': row['ì‚¬ê±´ëª…']
        }
        doc = Document(page_content=str(row['íŒë¡€ë‚´ìš©']), metadata=metadata)
        docs.append(doc)
    print('Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ')
<<<<<<< HEAD
    # model_name = 'intfloat/multilingual-e5-large-instruct'
    embeddings = SentenceTransformerEmbeddings(model_name='nlpai-lab/KURE-v1',kwargs={"device": "cuda"})
    
    print('í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘...')
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=250, ##### 1500
        chunk_overlap=25 ##### 300
=======
    
    # ì„ë² ë”© ëª¨ë¸ ì„¤ì • (CUDA ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼)
    device = "cuda" if cuda_available else "cpu"
    embeddings = SentenceTransformerEmbeddings(model_name='nlpai-lab/KURE-v1', model_kwargs={"device": device})
    print(f"ğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ì¥ì¹˜: {device})")
    
    # ì²« ë²ˆì§¸ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (chunk_size=250, chunk_overlap=50)
    print('\n=== ì²« ë²ˆì§¸ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (chunk_size=250, chunk_overlap=50) ===')
    chunk_start_time = time.time()
    
    print('í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘...')
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=250,
        chunk_overlap=50
>>>>>>> c999bd342716fb314c0a6ed0f194555535ac501e
    )
    
    split_docs = text_splitter.split_documents(docs)
    print(f'í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ: {len(docs)}ê°œ ë¬¸ì„œ â†’ {len(split_docs)}ê°œ ì²­í¬')
    
    # ì˜ˆìƒ ì™„ë£Œì‹œê°„ ê³„ì‚°
    estimated_time_per_chunk = 0.1 if cuda_available else 0.5  # ì´ˆ ë‹¨ìœ„ (GPU/CPUì— ë”°ë¼ ë‹¤ë¦„)
    total_estimated_time = len(split_docs) * estimated_time_per_chunk
    estimated_completion = datetime.now() + timedelta(seconds=total_estimated_time)
    
    print(f"â±ï¸  ì˜ˆìƒ ì™„ë£Œì‹œê°„: {estimated_completion.strftime('%H:%M:%S')} (ì•½ {total_estimated_time/60:.1f}ë¶„)")
    
    # ìì‹ ì²­í¬ë¥¼ ì €ì¥í•  ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    print('ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...')
    total_docs = len(split_docs)
    print(f'ì´ {len(docs)}ê°œì˜ ë¬¸ì„œë¥¼ {total_docs}ê°œì˜ ì²­í¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤...')
    
    # ë°°ì¹˜ í¬ê¸° ì„¤ì • (ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ìœ„í•´)
    batch_size = 1000
    processed = 0
    
    for i in range(0, total_docs, batch_size):
        batch_start_time = time.time()
        batch_docs = split_docs[i:i + batch_size]
        processed += len(batch_docs)
        
        if i == 0:
            # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
            vectorstore = Chroma.from_documents(
                documents=batch_docs,
                embedding=embeddings,
                collection_name='LAW_RAG_250_50',
                persist_directory=base_db_dir
            )
        else:
            # ë‚˜ë¨¸ì§€ ë°°ì¹˜ëŠ” ì¶”ê°€
            vectorstore.add_documents(batch_docs)
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        batch_time = time.time() - batch_start_time
        remaining_chunks = total_docs - processed
        estimated_remaining_time = remaining_chunks * (batch_time / len(batch_docs))
        estimated_completion = datetime.now() + timedelta(seconds=estimated_remaining_time)
        
        print(f'ì§„í–‰ë¥ : {processed}/{total_docs} ({processed/total_docs*100:.1f}%) - ì˜ˆìƒ ì™„ë£Œ: {estimated_completion.strftime("%H:%M:%S")}')
    
    vectorstore.persist()
    chunk_time = time.time() - chunk_start_time
    print(f'âœ… ì²« ë²ˆì§¸ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {chunk_time/60:.1f}ë¶„)')

    # ë‘ ë²ˆì§¸ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (chunk_size=500, chunk_overlap=75)
    print('\n=== ë‘ ë²ˆì§¸ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (chunk_size=500, chunk_overlap=75) ===')
    chunk_start_time = time.time()
    
    print('ë‘ë²ˆì§¸ í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘...')
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=75
    )
    
    split_docs = text_splitter.split_documents(docs)
    print(f'í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ: {len(docs)}ê°œ ë¬¸ì„œ â†’ {len(split_docs)}ê°œ ì²­í¬')
    
    # ì˜ˆìƒ ì™„ë£Œì‹œê°„ ê³„ì‚°
    total_estimated_time = len(split_docs) * estimated_time_per_chunk
    estimated_completion = datetime.now() + timedelta(seconds=total_estimated_time)
    
    print(f"â±ï¸  ì˜ˆìƒ ì™„ë£Œì‹œê°„: {estimated_completion.strftime('%H:%M:%S')} (ì•½ {total_estimated_time/60:.1f}ë¶„)")
    
    # ìì‹ ì²­í¬ë¥¼ ì €ì¥í•  ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    print('ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...')
    total_docs = len(split_docs)
    print(f'ì´ {len(docs)}ê°œì˜ ë¬¸ì„œë¥¼ {total_docs}ê°œì˜ ì²­í¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤...')
    
    # ë°°ì¹˜ í¬ê¸° ì„¤ì •
    batch_size = 1000
    processed = 0
    
    for i in range(0, total_docs, batch_size):
        batch_start_time = time.time()
        batch_docs = split_docs[i:i + batch_size]
        processed += len(batch_docs)
        
        if i == 0:
            # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
            vectorstore = Chroma.from_documents(
                documents=batch_docs,
                embedding=embeddings,
                collection_name='LAW_RAG_500_75',
                persist_directory=base_db_dir
            )
            return
        else:
            # ë‚˜ë¨¸ì§€ ë°°ì¹˜ëŠ” ì¶”ê°€
            vectorstore.add_documents(batch_docs)
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        batch_time = time.time() - batch_start_time
        remaining_chunks = total_docs - processed
        estimated_remaining_time = remaining_chunks * (batch_time / len(batch_docs))
        estimated_completion = datetime.now() + timedelta(seconds=estimated_remaining_time)
        
        print(f'ì§„í–‰ë¥ : {processed}/{total_docs} ({processed/total_docs*100:.1f}%) - ì˜ˆìƒ ì™„ë£Œ: {estimated_completion.strftime("%H:%M:%S")}')
    
    vectorstore.persist()
    chunk_time = time.time() - chunk_start_time
    print(f'âœ… ë‘ ë²ˆì§¸ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {chunk_time/60:.1f}ë¶„)')
    
    # ì „ì²´ ì™„ë£Œ ì‹œê°„ ê³„ì‚°
    total_time = time.time() - start_time
    print(f"\nğŸ‰ ì „ì²´ ì‘ì—… ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ ì†Œìš”ì‹œê°„: {total_time/60:.1f}ë¶„")
    print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {base_db_dir}")
    print(f"[ì™„ë£Œ] íŒë¡€ {len(docs)}ê±´ì´ ì›ë³¸ê³¼ ì²­í¬ë¡œ ë¶„ë¦¬ë˜ì–´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


    
def retrieve_db(query,host,port,username,password,db_name,api_key,base_db_dir='./db'):
    print('ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...')
    vectorstore = Chroma(
        persist_directory=base_db_dir,
        embedding_function=SentenceTransformerEmbeddings(model_name='nlpai-lab/KURE-v1',kwargs={"device": "cuda"}),
        collection_name='LAW_RAG'
    )
    print('ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ')
    retriever = vectorstore.as_retriever(search_kwargs={"k": 1})
    print('ë²¡í„°ìŠ¤í† ì–´ ê²€ìƒ‰ ì¤‘...')
    results = retriever.invoke(query)
    
    conn = get_mysql_connection(host,port,username,password,db_name)
    # ê²°ê³¼ ì¶œë ¥
    for i, doc in enumerate(results):
        meta = doc.metadata
        print(f"\nğŸ” [ê²°ê³¼ {i+1}]")
        print(f"â–¶ íŒë¡€ì¼ë ¨ë²ˆí˜¸ : {meta['source']}")
        print(f"â–¶ ì‚¬ê±´ëª… : {meta['case_type']}")
        print("â–¶ ìœ ì‚¬ ë¬¸ë‹¨:", doc.page_content.strip())
        result = get_document(conn,meta['source'])
        print('â–¶ ì „ì²´ íŒë¡€:',result['íŒë¡€ë‚´ìš©'])
        print("\n" + "="*50)