from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings   
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.retrievers import ParentDocumentRetriever
from langchain.schema import Document
import pandas as pd
import pymysql
from langchain_community.storage import SQLStore
from langchain_huggingface import HuggingFaceEmbeddings
import os
import warnings
import chromadb
warnings.filterwarnings('ignore')


def get_mysql_connection(host,port,username,password,db_name):
    try:
        conn = pymysql.connect(host=host, port=port, user=username, password=password, db=db_name,charset='utf8',cursorclass=pymysql.cursors.DictCursor)
        print(f"MySQL ì—°ê²° ì„±ê³µ")
        return conn
    except Exception as e:
        print(f"MySQL ì—°ê²° ì˜¤ë¥˜: {e}")
        return None

def get_document(conn,source):
    try: 
        with conn.cursor() as cursor:
            sql = f'SELECT íŒë¡€ì¼ë ¨ë²ˆí˜¸, íŒë¡€ë‚´ìš© FROM íŒë¡€ WHERE íŒë¡€ì¼ë ¨ë²ˆí˜¸ = {source}'
            cursor.execute(sql)
            result = cursor.fetchone()
            return result
    except Exception as e:
        print(f"MySQL ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return None



def save_df(host,port,username,password,db_name):
    print("íŒë¡€ í…Œì´ë¸”ì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ì˜¤ê³  ìˆìŠµë‹ˆë‹¤...")
    conn = pymysql.connect(host=host, port=port, user=username, password=password, db=db_name)
    df = pd.read_sql('SELECT * FROM íŒë¡€', conn)
    print(f"ì´ {len(df)}ê±´ì˜ íŒë¡€ë¥¼ ì½ì—ˆìŠµë‹ˆë‹¤.")
    df.to_csv('íŒë¡€.csv', index=False)
    print("csv íŒŒì¼ë¡œ ì €ì¥ ì™„ë£Œ: íŒë¡€.csv")
    conn.close()

def create_db(api_key,base_db_dir='./db_test'):
    # íŒë¡€.csv íŒŒì¼ ì½ê¸°
    df_íŒë¡€ = pd.read_csv('íŒë¡€.csv')

    
    # ê¸°ì¡´ ë²¡í„° DBê°€ ì¡´ì¬í•˜ë©´ ì‚­ì œ
    try:
        client = chromadb.PersistentClient(path=base_db_dir)
        client.delete_collection(name='LAW_RAG_TEST_1000_100_openai')
        print(f"[ì‚­ì œ] ê¸°ì¡´ ë²¡í„° DBê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤: {base_db_dir}")
    except Exception as e:
        print(f"[ê²½ê³ ] ê¸°ì¡´ DB ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    # DB ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(base_db_dir, exist_ok=True)
    
    # Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    print('Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ ìƒì„± ì¤‘...')
    docs = []
    for i, row in df_íŒë¡€.iterrows():
        metadata = {
            "source": row['íŒë¡€ì¼ë ¨ë²ˆí˜¸'],
            'case_type': row['ì‚¬ê±´ëª…']
        }
        doc = Document(page_content=str(row['íŒë¡€ë‚´ìš©']), metadata=metadata)
        docs.append(doc)
    print('Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ')
    # model_name = 'intfloat/multilingual-e5-large-instruct'
    embeddings = OpenAIEmbeddings(api_key=api_key)
    
    print('í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘...')
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=250, ##### 1500
        chunk_overlap=50 ##### 300
    )
    
    split_docs = text_splitter.split_documents(docs)
    print('í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ')
    
    # ìì‹ ì²­í¬ë¥¼ ì €ì¥í•  ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    print('ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...')
    total_docs = len(split_docs)
    print(f'ì´ {len(split_docs)}ê°œì˜ ë¬¸ì„œë¥¼ {total_docs}ê°œì˜ ì²­í¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤...')
    
    # ë°°ì¹˜ í¬ê¸° ì„¤ì • (ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ìœ„í•´)
    batch_size = 1000
    processed = 0
    
    for i in range(0, total_docs, batch_size):
        batch_docs = split_docs[i:i + batch_size]
        processed += len(batch_docs)
        
        if i == 0:
            # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
            vectorstore = Chroma.from_documents(
                documents=batch_docs,
                embedding=embeddings,
                collection_name='LAW_RAG_TEST_100_100_openai',
                persist_directory=base_db_dir
            )
        else:
            # ë‚˜ë¨¸ì§€ ë°°ì¹˜ëŠ” ì¶”ê°€
            vectorstore.add_documents(batch_docs)
        
        print(f'ì§„í–‰ë¥ : {processed}/{total_docs} ({processed/total_docs*100:.1f}%)')
    
    print('ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ!')

    vectorstore.persist()
    
    print(f"[ì™„ë£Œ] íŒë¡€ {len(docs)}ê±´ì´ ì›ë³¸ê³¼ ì²­í¬ë¡œ ë¶„ë¦¬ë˜ì–´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ì›ë³¸ ë¬¸ì„œ: SQL í…Œì´ë¸” 'íŒë¡€_ì›ë³¸'")
    print(f"ì²­í¬ ë²¡í„°: {base_db_dir}/LAW_RAG")
    
def retrieve_db(query,host,port,username,password,db_name,api_key,base_db_dir='./db'):
    print('ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...')
    vectorstore = Chroma(
        persist_directory=base_db_dir,
        embedding_function=OpenAIEmbeddings(api_key=api_key),
        collection_name='LAW_RAG_TEST_1000_100_openai'
    )
    print('ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ')
    retriever = vectorstore.as_retriever(search_kwargs={"k": 1})
    print('ë²¡í„°ìŠ¤í† ì–´ ê²€ìƒ‰ ì¤‘...')
    results = retriever.invoke(query)
    print("retriever.invoke ê²°ê³¼:", results)
    
    conn = get_mysql_connection(host,port,username,password,db_name)
    output = []
    # ê²°ê³¼ ì¶œë ¥
    for i, doc in enumerate(results):
        meta = doc.metadata
        result = get_document(conn, meta['source'])
        output.append({
            "rank": i+1,
            "íŒë¡€ì¼ë ¨ë²ˆí˜¸": meta['source'],
            "ì‚¬ê±´ëª…": meta.get('case_type'),
            "ìœ ì‚¬ë¬¸ë‹¨": doc.page_content.strip(),
            "ì „ë¬¸": result['íŒë¡€ë‚´ìš©'] if result else None
        })
        print(f"\nğŸ” [ê²°ê³¼ {i+1}]")
        print(f"â–¶ íŒë¡€ì¼ë ¨ë²ˆí˜¸ : {meta['source']}")
        print(f"â–¶ ì‚¬ê±´ëª… : {meta['case_type']}")
        print("â–¶ ìœ ì‚¬ ë¬¸ë‹¨:", doc.page_content.strip())
        print("â–¶ ìœ ì‚¬ íŒë¡€ ID :", doc.metadata["source"])
        print("â–¶ ì‚¬ê±´ëª…       :", doc.metadata["case_type"])
        result = get_document(conn,meta['source'])
        print('â–¶ ì „ì²´ íŒë¡€:',result['íŒë¡€ë‚´ìš©'])
        print("\n" + "="*50)
    return output
