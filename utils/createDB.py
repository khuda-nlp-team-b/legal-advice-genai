# main.py
import os
from dotenv import load_dotenv
import util as u
import pymysql
import pandas as pd
import torch
import time
from datetime import datetime, timedelta
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain.embeddings import SentenceTransformerEmbeddings

load_dotenv()


def save_df(host,port,username,password,db_name):
    conn = pymysql.connect(host=host, port=port, user=username, password=password, db=db_name)
    df = pd.read_sql('SELECT * FROM íŒë¡€', conn)
    df.to_csv('íŒë¡€.csv', index=False)
    conn.close()

def create_db_from_mysql(host, port, username, password, db_name, base_db_dir='./db'):
    """MySQLì—ì„œ ì§ì ‘ ë°ì´í„°ë¥¼ ì½ì–´ì„œ ë²¡í„° DBë¥¼ êµ¬ì¶•"""
    print("ğŸ”„ MySQLì—ì„œ íŒë¡€ ë°ì´í„°ë¥¼ ì½ì–´ì˜¤ëŠ” ì¤‘...")
    
    # MySQL ì—°ê²° ë° ë°ì´í„° ì½ê¸°
    conn = pymysql.connect(host=host, port=port, user=username, password=password, db=db_name)
    df_íŒë¡€ = pd.read_sql('SELECT * FROM íŒë¡€', conn)
    conn.close()
    
    print(f"ğŸ“Š ì´ {len(df_íŒë¡€)}ê°œì˜ íŒë¡€ ë°ì´í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤")
    
    # CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    cuda_available = torch.cuda.is_available()
    if cuda_available:
        print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
        print(f"   GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    else:
        print("âŒ CUDA ì‚¬ìš© ë¶ˆê°€ëŠ¥ - CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤")
    
    # ì‹œì‘ ì‹œê°„ ê¸°ë¡
    start_time = time.time()
    
    # ê¸°ì¡´ ë²¡í„° DB ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ë©´ ì™„ì „ ì‚­ì œ
    if os.path.exists(base_db_dir):
        try:
            import shutil
            shutil.rmtree(base_db_dir)
            print(f"[ì‚­ì œ] ê¸°ì¡´ ë²¡í„° DB ë””ë ‰í† ë¦¬ê°€ ì™„ì „íˆ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤: {base_db_dir}")
        except Exception as e:
            print(f"[ê²½ê³ ] ê¸°ì¡´ DB ë””ë ‰í† ë¦¬ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    # DB ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(base_db_dir, exist_ok=True)
    
    # Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    print('Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ ìƒì„± ì¤‘...')
    docs = []
    for i, row in df_íŒë¡€.iterrows():
        metadata = {
            "source": row['íŒë¡€ì¼ë ¨ë²ˆí˜¸'],
            'case_type': row['ì‚¬ê±´ëª…']
        }
        doc = Document(page_content=str(row['íŒë¡€ë‚´ìš©']), metadata=metadata)
        docs.append(doc)
    print('Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ')
    
    # ì„ë² ë”© ëª¨ë¸ ì„¤ì • (CUDA ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼)
    device = "cuda" if cuda_available else "cpu"
    embeddings = SentenceTransformerEmbeddings(model_name='nlpai-lab/KURE-v1', model_kwargs={"device": device})
    print(f"ğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ì¥ì¹˜: {device})")
    
    # ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (chunk_size=500, chunk_overlap=75)
    print('\n=== ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (chunk_size=500, chunk_overlap=75) ===')
    chunk_start_time = time.time()
    
    print('í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘...')
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=75
    )
    
    split_docs = text_splitter.split_documents(docs)
    print(f'í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ: {len(docs)}ê°œ ë¬¸ì„œ â†’ {len(split_docs)}ê°œ ì²­í¬')
    
    # ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    print('ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...')
    total_docs = len(split_docs)
    print(f'ì´ {len(docs)}ê°œì˜ ë¬¸ì„œë¥¼ {total_docs}ê°œì˜ ì²­í¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤...')
    
    # ë°°ì¹˜ í¬ê¸° ì„¤ì • (ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ìœ„í•´)
    batch_size = 1000
    processed = 0
    processing_times = []  # ê° ë°°ì¹˜ì˜ ì²˜ë¦¬ ì‹œê°„ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    
    for i in range(0, total_docs, batch_size):
        batch_start_time = time.time()
        batch_docs = split_docs[i:i + batch_size]
        processed += len(batch_docs)
        
        if i == 0:
            # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
            vectorstore = Chroma.from_documents(
                documents=batch_docs,
                embedding=embeddings,
                collection_name='LAW_RAG_500_75',
                persist_directory=base_db_dir
            )
        else:
            # ë‚˜ë¨¸ì§€ ë°°ì¹˜ëŠ” ì¶”ê°€
            vectorstore.add_documents(batch_docs)
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        batch_time = time.time() - batch_start_time
        processing_times.append(batch_time)
        
        # ì´ì „ ë°°ì¹˜ë“¤ì˜ í‰ê·  ì²˜ë¦¬ ì‹œê°„ìœ¼ë¡œ ì˜ˆìƒ ì™„ë£Œì‹œê°„ ê³„ì‚°
        if len(processing_times) > 1:
            avg_time_per_batch = sum(processing_times) / len(processing_times)
            remaining_batches = (total_docs - processed) // batch_size + (1 if (total_docs - processed) % batch_size > 0 else 0)
            estimated_remaining_time = remaining_batches * avg_time_per_batch
            estimated_completion = datetime.now() + timedelta(seconds=estimated_remaining_time)
            
            print(f'ì§„í–‰ë¥ : {processed}/{total_docs} ({processed/total_docs*100:.1f}%) - ì˜ˆìƒ ì™„ë£Œ: {estimated_completion.strftime("%H:%M:%S")} (í‰ê·  ë°°ì¹˜ì‹œê°„: {avg_time_per_batch:.1f}ì´ˆ)')
        else:
            # ì²« ë²ˆì§¸ ë°°ì¹˜ í›„ì—ëŠ” ì•„ì§ í‰ê· ì„ ê³„ì‚°í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ í˜„ì¬ ë°°ì¹˜ ì‹œê°„ìœ¼ë¡œ ì¶”ì •
            remaining_batches = (total_docs - processed) // batch_size + (1 if (total_docs - processed) % batch_size > 0 else 0)
            estimated_remaining_time = remaining_batches * batch_time
            estimated_completion = datetime.now() + timedelta(seconds=estimated_remaining_time)
            
            print(f'ì§„í–‰ë¥ : {processed}/{total_docs} ({processed/total_docs*100:.1f}%) - ì˜ˆìƒ ì™„ë£Œ: {estimated_completion.strftime("%H:%M:%S")} (í˜„ì¬ ë°°ì¹˜ì‹œê°„: {batch_time:.1f}ì´ˆ)')
    
    vectorstore.persist()
    chunk_time = time.time() - chunk_start_time
    print(f'âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {chunk_time/60:.1f}ë¶„)')
    
    total_time = time.time() - start_time
    print(f'ğŸ‰ ì „ì²´ ì‘ì—… ì™„ë£Œ! (ì´ ì†Œìš”ì‹œê°„: {total_time/60:.1f}ë¶„)')

def create_db(base_db_dir='C:\\db'):
    # CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    cuda_available = torch.cuda.is_available()
    if cuda_available:
        print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
        print(f"   GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    else:
        print("âŒ CUDA ì‚¬ìš© ë¶ˆê°€ëŠ¥ - CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤")
    
    # ì‹œì‘ ì‹œê°„ ê¸°ë¡
    start_time = time.time()
    
    # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ íŒŒì¼ ê²½ë¡œ ì„¤ì •
    script_dir = os.path.dirname(os.path.abspath(__file__))
    csv_file_path = os.path.join(script_dir, 'íŒë¡€.csv')
    
    # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if not os.path.exists(csv_file_path):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_file_path}")
        print(f"í˜„ì¬ ë””ë ‰í† ë¦¬: {os.getcwd()}")
        print(f"ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬: {script_dir}")
        print("ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì¼ë“¤:")
        for file in os.listdir(script_dir):
            if file.endswith('.csv'):
                print(f"  - {file}")
        return
    
    # íŒë¡€.csv íŒŒì¼ ì½ê¸°
    print(f"ğŸ“ íŒŒì¼ ê²½ë¡œ: {csv_file_path}")
    df_íŒë¡€ = pd.read_csv(csv_file_path)
    print(f"ğŸ“Š ì´ {len(df_íŒë¡€)}ê°œì˜ íŒë¡€ ë°ì´í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤")

    # ê¸°ì¡´ ë²¡í„° DB ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ë©´ ì™„ì „ ì‚­ì œ
    if os.path.exists(base_db_dir):
        try:
            import shutil
            shutil.rmtree(base_db_dir)
            print(f"[ì‚­ì œ] ê¸°ì¡´ ë²¡í„° DB ë””ë ‰í† ë¦¬ê°€ ì™„ì „íˆ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤: {base_db_dir}")
        except Exception as e:
            print(f"[ê²½ê³ ] ê¸°ì¡´ DB ë””ë ‰í† ë¦¬ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    # DB ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(base_db_dir, exist_ok=True)
    
    # Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    print('Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ ìƒì„± ì¤‘...')
    docs = []
    for i, row in df_íŒë¡€.iterrows():
        metadata = {
            "source": row['íŒë¡€ì¼ë ¨ë²ˆí˜¸'],
            'case_type': row['ì‚¬ê±´ëª…']
        }
        doc = Document(page_content=str(row['íŒë¡€ë‚´ìš©']), metadata=metadata)
        docs.append(doc)
    print('Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ')
    # model_name = 'intfloat/multilingual-e5-large-instruct'
    embeddings = SentenceTransformerEmbeddings(model_name='nlpai-lab/KURE-v1', model_kwargs={"device": "cpu"})
    
    print('í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘...')
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=250, ##### 1500
        chunk_overlap=25 ##### 300
    )
    split_docs = text_splitter.split_documents(docs)
    print(f'í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ: {len(docs)}ê°œ ë¬¸ì„œ â†’ {len(split_docs)}ê°œ ì²­í¬')
    
    # ì„ë² ë”© ëª¨ë¸ ì„¤ì • (CUDA ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼)
    device = "cuda" if cuda_available else "cpu"
    embeddings = SentenceTransformerEmbeddings(model_name='nlpai-lab/KURE-v1', model_kwargs={"device": device})
    print(f"ğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ì¥ì¹˜: {device})")
    
    # ì²« ë²ˆì§¸ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (chunk_size=250, chunk_overlap=50)
    print('\n=== ì²« ë²ˆì§¸ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (chunk_size=250, chunk_overlap=50) ===')
    chunk_start_time = time.time()
    
    print('í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘...')
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=250,
        chunk_overlap=50
    )
    
    split_docs = text_splitter.split_documents(docs)
    print(f'í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ: {len(docs)}ê°œ ë¬¸ì„œ â†’ {len(split_docs)}ê°œ ì²­í¬')
    
    # ìì‹ ì²­í¬ë¥¼ ì €ì¥í•  ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    print('ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...')
    total_docs = len(split_docs)
    print(f'ì´ {len(docs)}ê°œì˜ ë¬¸ì„œë¥¼ {total_docs}ê°œì˜ ì²­í¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤...')
    
    # ë°°ì¹˜ í¬ê¸° ì„¤ì • (ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ìœ„í•´)
    batch_size = 1000
    processed = 0
    processing_times = []  # ê° ë°°ì¹˜ì˜ ì²˜ë¦¬ ì‹œê°„ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    
    for i in range(0, total_docs, batch_size):
        batch_start_time = time.time()
        batch_docs = split_docs[i:i + batch_size]
        processed += len(batch_docs)
        
        if i == 0:
            # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
            vectorstore = Chroma.from_documents(
                documents=batch_docs,
                embedding=embeddings,
                collection_name='LAW_RAG_250_50',
                persist_directory=base_db_dir
            )
        else:
            # ë‚˜ë¨¸ì§€ ë°°ì¹˜ëŠ” ì¶”ê°€
            vectorstore.add_documents(batch_docs)
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        batch_time = time.time() - batch_start_time
        processing_times.append(batch_time)
        
        # ì´ì „ ë°°ì¹˜ë“¤ì˜ í‰ê·  ì²˜ë¦¬ ì‹œê°„ìœ¼ë¡œ ì˜ˆìƒ ì™„ë£Œì‹œê°„ ê³„ì‚°
        if len(processing_times) > 1:
            avg_time_per_batch = sum(processing_times) / len(processing_times)
            remaining_batches = (total_docs - processed) // batch_size + (1 if (total_docs - processed) % batch_size > 0 else 0)
            estimated_remaining_time = remaining_batches * avg_time_per_batch
            estimated_completion = datetime.now() + timedelta(seconds=estimated_remaining_time)
            
            print(f'ì§„í–‰ë¥ : {processed}/{total_docs} ({processed/total_docs*100:.1f}%) - ì˜ˆìƒ ì™„ë£Œ: {estimated_completion.strftime("%H:%M:%S")} (í‰ê·  ë°°ì¹˜ì‹œê°„: {avg_time_per_batch:.1f}ì´ˆ)')
        else:
            # ì²« ë²ˆì§¸ ë°°ì¹˜ í›„ì—ëŠ” ì•„ì§ í‰ê· ì„ ê³„ì‚°í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ í˜„ì¬ ë°°ì¹˜ ì‹œê°„ìœ¼ë¡œ ì¶”ì •
            remaining_batches = (total_docs - processed) // batch_size + (1 if (total_docs - processed) % batch_size > 0 else 0)
            estimated_remaining_time = remaining_batches * batch_time
            estimated_completion = datetime.now() + timedelta(seconds=estimated_remaining_time)
            
            print(f'ì§„í–‰ë¥ : {processed}/{total_docs} ({processed/total_docs*100:.1f}%) - ì˜ˆìƒ ì™„ë£Œ: {estimated_completion.strftime("%H:%M:%S")} (í˜„ì¬ ë°°ì¹˜ì‹œê°„: {batch_time:.1f}ì´ˆ)')
    
    vectorstore.persist()
    chunk_time = time.time() - chunk_start_time
    print(f'âœ… ì²« ë²ˆì§¸ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {chunk_time/60:.1f}ë¶„)')

    # ë‘ ë²ˆì§¸ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (chunk_size=500, chunk_overlap=75)
    print('\n=== ë‘ ë²ˆì§¸ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (chunk_size=500, chunk_overlap=75) ===')
    chunk_start_time = time.time()
    
    print('ë‘ë²ˆì§¸ í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘...')
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=75
    )
    
    split_docs = text_splitter.split_documents(docs)
    print(f'í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ: {len(docs)}ê°œ ë¬¸ì„œ â†’ {len(split_docs)}ê°œ ì²­í¬')
    
    # ìì‹ ì²­í¬ë¥¼ ì €ì¥í•  ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    print('ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...')
    total_docs = len(split_docs)
    print(f'ì´ {len(docs)}ê°œì˜ ë¬¸ì„œë¥¼ {total_docs}ê°œì˜ ì²­í¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤...')
    
    # ë°°ì¹˜ í¬ê¸° ì„¤ì •
    batch_size = 1000
    processed = 0
    processing_times = []  # ê° ë°°ì¹˜ì˜ ì²˜ë¦¬ ì‹œê°„ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    
    for i in range(0, total_docs, batch_size):
        batch_start_time = time.time()
        batch_docs = split_docs[i:i + batch_size]
        processed += len(batch_docs)
        
        if i == 0:
            # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
            vectorstore = Chroma.from_documents(
                documents=batch_docs,
                embedding=embeddings,
                collection_name='LAW_RAG_500_75',
                persist_directory=base_db_dir
            )
            return
        else:
            # ë‚˜ë¨¸ì§€ ë°°ì¹˜ëŠ” ì¶”ê°€
            vectorstore.add_documents(batch_docs)
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        batch_time = time.time() - batch_start_time
        processing_times.append(batch_time)
        
        # ì´ì „ ë°°ì¹˜ë“¤ì˜ í‰ê·  ì²˜ë¦¬ ì‹œê°„ìœ¼ë¡œ ì˜ˆìƒ ì™„ë£Œì‹œê°„ ê³„ì‚°
        if len(processing_times) > 1:
            avg_time_per_batch = sum(processing_times) / len(processing_times)
            remaining_batches = (total_docs - processed) // batch_size + (1 if (total_docs - processed) % batch_size > 0 else 0)
            estimated_remaining_time = remaining_batches * avg_time_per_batch
            estimated_completion = datetime.now() + timedelta(seconds=estimated_remaining_time)
            
            print(f'ì§„í–‰ë¥ : {processed}/{total_docs} ({processed/total_docs*100:.1f}%) - ì˜ˆìƒ ì™„ë£Œ: {estimated_completion.strftime("%H:%M:%S")} (í‰ê·  ë°°ì¹˜ì‹œê°„: {avg_time_per_batch:.1f}ì´ˆ)')
        else:
            # ì²« ë²ˆì§¸ ë°°ì¹˜ í›„ì—ëŠ” ì•„ì§ í‰ê· ì„ ê³„ì‚°í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ í˜„ì¬ ë°°ì¹˜ ì‹œê°„ìœ¼ë¡œ ì¶”ì •
            remaining_batches = (total_docs - processed) // batch_size + (1 if (total_docs - processed) % batch_size > 0 else 0)
            estimated_remaining_time = remaining_batches * batch_time
            estimated_completion = datetime.now() + timedelta(seconds=estimated_remaining_time)
            
            print(f'ì§„í–‰ë¥ : {processed}/{total_docs} ({processed/total_docs*100:.1f}%) - ì˜ˆìƒ ì™„ë£Œ: {estimated_completion.strftime("%H:%M:%S")} (í˜„ì¬ ë°°ì¹˜ì‹œê°„: {batch_time:.1f}ì´ˆ)')
    
    vectorstore.persist()
    chunk_time = time.time() - chunk_start_time
    print(f'âœ… ë‘ ë²ˆì§¸ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {chunk_time/60:.1f}ë¶„)')
    
    # ì „ì²´ ì™„ë£Œ ì‹œê°„ ê³„ì‚°
    total_time = time.time() - start_time
    print(f"\nğŸ‰ ì „ì²´ ì‘ì—… ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ ì†Œìš”ì‹œê°„: {total_time/60:.1f}ë¶„")
    print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {base_db_dir}")
    print(f"[ì™„ë£Œ] íŒë¡€ {len(docs)}ê±´ì´ ì›ë³¸ê³¼ ì²­í¬ë¡œ ë¶„ë¦¬ë˜ì–´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

host = os.environ.get('DB_HOST')
port = 3306
username = os.environ.get('DB_USER')
password = os.getenv('DB_PASSWORD')
db = os.getenv('DB_NAME')
api_key = os.getenv('OPENAI_API_KEY')

#cdb.save_df(host,port,username,password,db)
#cdb.create_db()
#u.retrieve_db('íƒì‹œë¥¼ íƒ€ê³  ê°€ë‹¤ê°€, ìš´ì „ìê°€ ì‹ í˜¸ë¥¼ ìœ„ë°˜í•˜ë©° ë‹¤ë¥¸ ì°¨ëŸ‰ê³¼ ì¶©ëŒí•˜ëŠ” ì‚¬ê³ ê°€ ë°œìƒí–ˆë‹¤. íƒì‹œ íšŒì‚¬ëŠ” ì‚¬ê³ ì— ëŒ€í•œ ì±…ì„ì„ ì¸ì •í•˜ì§€ ì•Šê³  ìˆë‹¤.',host,port,username,password,db,k=3)

if __name__ == "__main__":
    # MySQLì—ì„œ ì§ì ‘ ë²¡í„° DB êµ¬ì¶• (ê¶Œì¥)
    create_db_from_mysql(
        host='woodjudge-db.cp0w6o200jli.ap-northeast-2.rds.amazonaws.com',
        port=3306,
        username='admin',
        password='woodjudgedbpw',
        db_name='woodjudgeDB',
        base_db_dir='./db'
    )