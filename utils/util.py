from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings   
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.schema import Document
import pandas as pd
import pymysql
import os
import warnings
import chromadb
<<<<<<< HEAD
from langchain.embeddings import SentenceTransformerEmbeddings
=======
from langchain_community.embeddings import SentenceTransformerEmbeddings
>>>>>>> 37c4389 (ë°°í¬ ì „ ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ)
import torch
import time
from datetime import datetime, timedelta
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
<<<<<<< HEAD
=======
import asyncio
from langchain.chains import ConversationChain  
from langchain.memory import ConversationBufferMemory

# LangChain í…”ë ˆë©”íŠ¸ë¦¬ ë¹„í™œì„±í™”
os.environ["LANGCHAIN_TRACING_V2"] = "false"
os.environ["LANGCHAIN_ENDPOINT"] = ""
os.environ["LANGCHAIN_API_KEY"] = ""
>>>>>>> 37c4389 (ë°°í¬ ì „ ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ)

warnings.filterwarnings('ignore')


def get_mysql_connection(host,port,username,password,db_name):
    try:
        conn = pymysql.connect(host=host, port=port, user=username, password=password, db=db_name,charset='utf8',cursorclass=pymysql.cursors.DictCursor)
        print(f"MySQL ì—°ê²° ì„±ê³µ")
        return conn
    except Exception as e:
        print(f"MySQL ì—°ê²° ì˜¤ë¥˜: {e}")
        return None

def get_document(conn,source):
    try: 
        with conn.cursor() as cursor:
            sql = f'SELECT íŒë¡€ì¼ë ¨ë²ˆí˜¸, íŒë¡€ë‚´ìš© FROM íŒë¡€ WHERE íŒë¡€ì¼ë ¨ë²ˆí˜¸ = {source}'
            cursor.execute(sql)
            result = cursor.fetchone()
            return result
    except Exception as e:
        print(f"MySQL ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return None


<<<<<<< HEAD

def save_df(host,port,username,password,db_name):
    conn = pymysql.connect(host=host, port=port, user=username, password=password, db=db_name)
    df = pd.read_sql('SELECT * FROM íŒë¡€', conn)
    df.to_csv('íŒë¡€.csv', index=False)
    conn.close()
    

def create_db(base_db_dir='C:\\db'):
    # CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    cuda_available = torch.cuda.is_available()
    if cuda_available:
        print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
        print(f"   GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    else:
        print("âŒ CUDA ì‚¬ìš© ë¶ˆê°€ëŠ¥ - CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤")
    
    # ì‹œì‘ ì‹œê°„ ê¸°ë¡
    start_time = time.time()
    
    # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ íŒŒì¼ ê²½ë¡œ ì„¤ì •
    script_dir = os.path.dirname(os.path.abspath(__file__))
    csv_file_path = os.path.join(script_dir, 'íŒë¡€.csv')
    
    # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if not os.path.exists(csv_file_path):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_file_path}")
        print(f"í˜„ì¬ ë””ë ‰í† ë¦¬: {os.getcwd()}")
        print(f"ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬: {script_dir}")
        print("ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì¼ë“¤:")
        for file in os.listdir(script_dir):
            if file.endswith('.csv'):
                print(f"  - {file}")
        return
    
    # íŒë¡€.csv íŒŒì¼ ì½ê¸°
    print(f"ğŸ“ íŒŒì¼ ê²½ë¡œ: {csv_file_path}")
    df_íŒë¡€ = pd.read_csv(csv_file_path)
    print(f"ğŸ“Š ì´ {len(df_íŒë¡€)}ê°œì˜ íŒë¡€ ë°ì´í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤")

    # ê¸°ì¡´ ë²¡í„° DB ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ë©´ ì™„ì „ ì‚­ì œ
    if os.path.exists(base_db_dir):
        try:
            import shutil
            shutil.rmtree(base_db_dir)
            print(f"[ì‚­ì œ] ê¸°ì¡´ ë²¡í„° DB ë””ë ‰í† ë¦¬ê°€ ì™„ì „íˆ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤: {base_db_dir}")
        except Exception as e:
            print(f"[ê²½ê³ ] ê¸°ì¡´ DB ë””ë ‰í† ë¦¬ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    # DB ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(base_db_dir, exist_ok=True)
    
    # Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    print('Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ ìƒì„± ì¤‘...')
    docs = []
    for i, row in df_íŒë¡€.iterrows():
        metadata = {
            "source": row['íŒë¡€ì¼ë ¨ë²ˆí˜¸'],
            'case_type': row['ì‚¬ê±´ëª…']
        }
        doc = Document(page_content=str(row['íŒë¡€ë‚´ìš©']), metadata=metadata)
        docs.append(doc)
    print('Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ')
    # model_name = 'intfloat/multilingual-e5-large-instruct'
    embeddings = SentenceTransformerEmbeddings(model_name='nlpai-lab/KURE-v1', model_kwargs={"device": "cpu"})
    
    print('í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘...')
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=250, ##### 1500
        chunk_overlap=25 ##### 300
    )
    split_docs = text_splitter.split_documents(docs)
    print(f'í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ: {len(docs)}ê°œ ë¬¸ì„œ â†’ {len(split_docs)}ê°œ ì²­í¬')
    
    # ì„ë² ë”© ëª¨ë¸ ì„¤ì • (CUDA ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼)
    device = "cuda" if cuda_available else "cpu"
    embeddings = SentenceTransformerEmbeddings(model_name='nlpai-lab/KURE-v1', model_kwargs={"device": device})
    print(f"ğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ì¥ì¹˜: {device})")
    
    # ì²« ë²ˆì§¸ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (chunk_size=250, chunk_overlap=50)
    print('\n=== ì²« ë²ˆì§¸ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (chunk_size=250, chunk_overlap=50) ===')
    chunk_start_time = time.time()
    
    print('í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘...')
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=250,
        chunk_overlap=50
    )
    
    split_docs = text_splitter.split_documents(docs)
    print(f'í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ: {len(docs)}ê°œ ë¬¸ì„œ â†’ {len(split_docs)}ê°œ ì²­í¬')
    
    # ìì‹ ì²­í¬ë¥¼ ì €ì¥í•  ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    print('ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...')
    total_docs = len(split_docs)
    print(f'ì´ {len(docs)}ê°œì˜ ë¬¸ì„œë¥¼ {total_docs}ê°œì˜ ì²­í¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤...')
    
    # ë°°ì¹˜ í¬ê¸° ì„¤ì • (ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ìœ„í•´)
    batch_size = 1000
    processed = 0
    processing_times = []  # ê° ë°°ì¹˜ì˜ ì²˜ë¦¬ ì‹œê°„ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    
    for i in range(0, total_docs, batch_size):
        batch_start_time = time.time()
        batch_docs = split_docs[i:i + batch_size]
        processed += len(batch_docs)
        
        if i == 0:
            # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
            vectorstore = Chroma.from_documents(
                documents=batch_docs,
                embedding=embeddings,
                collection_name='LAW_RAG_250_50',
                persist_directory=base_db_dir
            )
        else:
            # ë‚˜ë¨¸ì§€ ë°°ì¹˜ëŠ” ì¶”ê°€
            vectorstore.add_documents(batch_docs)
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        batch_time = time.time() - batch_start_time
        processing_times.append(batch_time)
        
        # ì´ì „ ë°°ì¹˜ë“¤ì˜ í‰ê·  ì²˜ë¦¬ ì‹œê°„ìœ¼ë¡œ ì˜ˆìƒ ì™„ë£Œì‹œê°„ ê³„ì‚°
        if len(processing_times) > 1:
            avg_time_per_batch = sum(processing_times) / len(processing_times)
            remaining_batches = (total_docs - processed) // batch_size + (1 if (total_docs - processed) % batch_size > 0 else 0)
            estimated_remaining_time = remaining_batches * avg_time_per_batch
            estimated_completion = datetime.now() + timedelta(seconds=estimated_remaining_time)
            
            print(f'ì§„í–‰ë¥ : {processed}/{total_docs} ({processed/total_docs*100:.1f}%) - ì˜ˆìƒ ì™„ë£Œ: {estimated_completion.strftime("%H:%M:%S")} (í‰ê·  ë°°ì¹˜ì‹œê°„: {avg_time_per_batch:.1f}ì´ˆ)')
        else:
            # ì²« ë²ˆì§¸ ë°°ì¹˜ í›„ì—ëŠ” ì•„ì§ í‰ê· ì„ ê³„ì‚°í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ í˜„ì¬ ë°°ì¹˜ ì‹œê°„ìœ¼ë¡œ ì¶”ì •
            remaining_batches = (total_docs - processed) // batch_size + (1 if (total_docs - processed) % batch_size > 0 else 0)
            estimated_remaining_time = remaining_batches * batch_time
            estimated_completion = datetime.now() + timedelta(seconds=estimated_remaining_time)
            
            print(f'ì§„í–‰ë¥ : {processed}/{total_docs} ({processed/total_docs*100:.1f}%) - ì˜ˆìƒ ì™„ë£Œ: {estimated_completion.strftime("%H:%M:%S")} (í˜„ì¬ ë°°ì¹˜ì‹œê°„: {batch_time:.1f}ì´ˆ)')
    
    vectorstore.persist()
    chunk_time = time.time() - chunk_start_time
    print(f'âœ… ì²« ë²ˆì§¸ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {chunk_time/60:.1f}ë¶„)')

    # ë‘ ë²ˆì§¸ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (chunk_size=500, chunk_overlap=75)
    print('\n=== ë‘ ë²ˆì§¸ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (chunk_size=500, chunk_overlap=75) ===')
    chunk_start_time = time.time()
    
    print('ë‘ë²ˆì§¸ í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘...')
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=75
    )
    
    split_docs = text_splitter.split_documents(docs)
    print(f'í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ: {len(docs)}ê°œ ë¬¸ì„œ â†’ {len(split_docs)}ê°œ ì²­í¬')
    
    # ìì‹ ì²­í¬ë¥¼ ì €ì¥í•  ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    print('ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...')
    total_docs = len(split_docs)
    print(f'ì´ {len(docs)}ê°œì˜ ë¬¸ì„œë¥¼ {total_docs}ê°œì˜ ì²­í¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤...')
    
    # ë°°ì¹˜ í¬ê¸° ì„¤ì •
    batch_size = 1000
    processed = 0
    processing_times = []  # ê° ë°°ì¹˜ì˜ ì²˜ë¦¬ ì‹œê°„ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    
    for i in range(0, total_docs, batch_size):
        batch_start_time = time.time()
        batch_docs = split_docs[i:i + batch_size]
        processed += len(batch_docs)
        
        if i == 0:
            # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
            vectorstore = Chroma.from_documents(
                documents=batch_docs,
                embedding=embeddings,
                collection_name='LAW_RAG_500_75',
                persist_directory=base_db_dir
            )
            return
        else:
            # ë‚˜ë¨¸ì§€ ë°°ì¹˜ëŠ” ì¶”ê°€
            vectorstore.add_documents(batch_docs)
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        batch_time = time.time() - batch_start_time
        processing_times.append(batch_time)
        
        # ì´ì „ ë°°ì¹˜ë“¤ì˜ í‰ê·  ì²˜ë¦¬ ì‹œê°„ìœ¼ë¡œ ì˜ˆìƒ ì™„ë£Œì‹œê°„ ê³„ì‚°
        if len(processing_times) > 1:
            avg_time_per_batch = sum(processing_times) / len(processing_times)
            remaining_batches = (total_docs - processed) // batch_size + (1 if (total_docs - processed) % batch_size > 0 else 0)
            estimated_remaining_time = remaining_batches * avg_time_per_batch
            estimated_completion = datetime.now() + timedelta(seconds=estimated_remaining_time)
            
            print(f'ì§„í–‰ë¥ : {processed}/{total_docs} ({processed/total_docs*100:.1f}%) - ì˜ˆìƒ ì™„ë£Œ: {estimated_completion.strftime("%H:%M:%S")} (í‰ê·  ë°°ì¹˜ì‹œê°„: {avg_time_per_batch:.1f}ì´ˆ)')
        else:
            # ì²« ë²ˆì§¸ ë°°ì¹˜ í›„ì—ëŠ” ì•„ì§ í‰ê· ì„ ê³„ì‚°í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ í˜„ì¬ ë°°ì¹˜ ì‹œê°„ìœ¼ë¡œ ì¶”ì •
            remaining_batches = (total_docs - processed) // batch_size + (1 if (total_docs - processed) % batch_size > 0 else 0)
            estimated_remaining_time = remaining_batches * batch_time
            estimated_completion = datetime.now() + timedelta(seconds=estimated_remaining_time)
            
            print(f'ì§„í–‰ë¥ : {processed}/{total_docs} ({processed/total_docs*100:.1f}%) - ì˜ˆìƒ ì™„ë£Œ: {estimated_completion.strftime("%H:%M:%S")} (í˜„ì¬ ë°°ì¹˜ì‹œê°„: {batch_time:.1f}ì´ˆ)')
    
    vectorstore.persist()
    chunk_time = time.time() - chunk_start_time
    print(f'âœ… ë‘ ë²ˆì§¸ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {chunk_time/60:.1f}ë¶„)')
    
    # ì „ì²´ ì™„ë£Œ ì‹œê°„ ê³„ì‚°
    total_time = time.time() - start_time
    print(f"\nğŸ‰ ì „ì²´ ì‘ì—… ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ ì†Œìš”ì‹œê°„: {total_time/60:.1f}ë¶„")
    print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {base_db_dir}")
    print(f"[ì™„ë£Œ] íŒë¡€ {len(docs)}ê±´ì´ ì›ë³¸ê³¼ ì²­í¬ë¡œ ë¶„ë¦¬ë˜ì–´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

=======
>>>>>>> 37c4389 (ë°°í¬ ì „ ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ)
def multiquery_retrieve_db(query,host,port,username,password,db_name,base_db_dir='./db',k=1):
    cuda_available = torch.cuda.is_available()
    if cuda_available:
        print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
        print(f"   GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    else:
        print("âŒ CUDA ì‚¬ìš© ë¶ˆê°€ëŠ¥ - CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤")
    device = "cuda" if cuda_available else "cpu"
    
    vectorstore = Chroma(
        persist_directory=base_db_dir,
        embedding_function=SentenceTransformerEmbeddings(model_name='nlpai-lab/KURE-v1', model_kwargs={"device": device}),
        collection_name='LAW_RAG_500_75'
    )
    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": k, "fetch_k": 20, "lambda_mult": 0.5}
    )
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=1)
    prompt = PromptTemplate.from_template(
        """
        By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of distance-based similarity search.  
<<<<<<< HEAD
Your response must be a list of values separated solely by new-line characters, e.g.  
foo\nbar\nbaz\n

#ORIGINAL QUESTION:  
{question}

#Answer in Korean:
### Role
You are a â€œLegal-document RAGâ€ multi-query generator.  
Based on the single incident scenario provided below, create **5** search queries that will surface a broad range of relevant case-law materials.

### Input format  
[Incident scenario]

### Output rules  
1. Output **one query per line**, max 120 characters (including spaces).  
2. Provide only the raw query stringsâ€”no numbering, quotes, comments, or explanations.  
3. Avoid duplicate or near-duplicate queries; use distinct wording and perspectives.  
4. Separate lines with `\n` only, following the pattern `foo\nbar\nbaz\n`.

### Query-writing guidelines  
- Restate the **key facts/acts** and **main legal issues** (tort, vicarious liability, bailment/custody, insurer subrogation, etc.) in varied ways.  
- Restate the situation in a legal way so that it can be used as a search query.
=======
        Your response must be a list of values separated solely by new-line characters, e.g.  
        foo\nbar\nbaz\n

        #ORIGINAL QUESTION:  
        {question}

        #Answer in Korean:
        ### Role
        You are a "Legal-document RAG" multi-query generator.  
        Based on the single incident scenario provided below, create **5** search queries that will surface a broad range of relevant case-law materials.

        ### Input format  
        [Incident scenario]

        ### Output rules  
        1. Output **one query per line**, max 120 characters (including spaces).  
        2. Provide only the raw query stringsâ€”no numbering, quotes, comments, or explanations.  
        3. Avoid duplicate or near-duplicate queries; use distinct wording and perspectives.  
        4. Separate lines with `\n` only, following the pattern `foo\nbar\nbaz\n`.

        ### Query-writing guidelines  
        - Restate the **key facts/acts** and **main legal issues** (tort, vicarious liability, bailment/custody, insurer subrogation, etc.) in varied ways.  
        - Restate the situation in a legal way so that it can be used as a search query.
>>>>>>> 37c4389 (ë°°í¬ ì „ ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ)

        """
    )
    multiquery_retriever = MultiQueryRetriever.from_llm(
        retriever=retriever,
        llm=llm,
        prompt = prompt,
        include_original=True,
    )
    results = multiquery_retriever.invoke(query)
    conn = get_mysql_connection(host,port,username,password,db_name)
    # ê²°ê³¼ ì¶œë ¥
    output = []
    for i, doc in enumerate(results):
        meta = doc.metadata
        result = get_document(conn, meta['source'])
        output.append({
            "rank": i+1,
            "íŒë¡€ì¼ë ¨ë²ˆí˜¸": meta['source'],
            "ì‚¬ê±´ëª…": meta.get('case_type'),
            "ìœ ì‚¬ë¬¸ë‹¨": doc.page_content.strip(),
            "ì „ë¬¸": result['íŒë¡€ë‚´ìš©'] if result else None
        })
    return output

    
<<<<<<< HEAD
def retrieve_db(query,host,port,username,password,db_name,vectorstore,k=1):
    
    retriever = vectorstore.as_retriever(
        search_type="mmr", 
        search_kwargs={"k": k, "fetch_k": 20, "lambda_mult": 0.5},
=======
def retrieve_db(query,conn,vectorstore,k=1,threshold=0.0):
    
    retriever = vectorstore.as_retriever(
        search_type="mmr", 
        search_kwargs={"k": k, "fetch_k": 20, "lambda_mult": 0.85, "score_threshold": threshold},
>>>>>>> 37c4389 (ë°°í¬ ì „ ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ)
        return_metadata=True
    )
    
    '''cross_encoder = HuggingFaceCrossEncoder(model_name="BAAI/bge-reranker-v2-m3")
    compressor = CrossEncoderReranker(model=cross_encoder,top_n=k)
    compression_retriever = ContextualCompressionRetriever(
        base_compressor=compressor,
        base_retriever=retriever
    )
    
    results = compression_retriever.invoke(query)'''
    results = retriever.invoke(query)
    
    print('ë²¡í„°ìŠ¤í† ì–´ ê²€ìƒ‰ ì¤‘...')

<<<<<<< HEAD
    if not conn:
        conn = get_mysql_connection(host,port,username,password,db_name)
=======
>>>>>>> 37c4389 (ë°°í¬ ì „ ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ)
    # ê²°ê³¼ ì¶œë ¥
    output = []
    for i, doc in enumerate(results):
        meta = doc.metadata
        result = get_document(conn, meta['source'])
        output.append({
        "rank": i+1,
        "íŒë¡€ì¼ë ¨ë²ˆí˜¸": meta['source'],
        "ì‚¬ê±´ëª…": meta.get('case_type'),
        "ìœ ì‚¬ë¬¸ë‹¨": doc.page_content.strip(),
        "ì „ë¬¸": result['íŒë¡€ë‚´ìš©'] if result else None
        })
        #print(meta['source'])
        #print(doc.page_content)
<<<<<<< HEAD
    conn.close()
=======
>>>>>>> 37c4389 (ë°°í¬ ì „ ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ)
    return output


def check_db(base_db_dir='./db'):
    print(f'ğŸ” DB ê²½ë¡œ: {base_db_dir}')
    try:
        client = chromadb.PersistentClient(path=base_db_dir)
        collections = client.list_collections()
        if not collections:
            print('âŒ ì»¬ë ‰ì…˜ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.')
            return
        print(f'âœ… {len(collections)}ê°œì˜ ì»¬ë ‰ì…˜ì´ ì¡´ì¬í•©ë‹ˆë‹¤:')
        for col in collections:
            print(f'\nğŸ“ ì»¬ë ‰ì…˜ ì´ë¦„: {col.name}')
            print(f'  - id: {col.id}')
            print(f'  - ë©”íƒ€ë°ì´í„°: {col.metadata}')
            print(f'  - document count: {col.count()}')
    except Exception as e:
        print(f'âŒ DB í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}')
    
def delete_collection(collection_name, base_db_dir='./db'):
    print(f'ğŸ—‘ï¸ ì»¬ë ‰ì…˜ ì‚­ì œ ì‹œë„: {collection_name} (DB ê²½ë¡œ: {base_db_dir})')
    try:
        client = chromadb.PersistentClient(path=base_db_dir)
        client.delete_collection(name=collection_name)
        print(f'âœ… ì»¬ë ‰ì…˜ "{collection_name}" ì‚­ì œ ì™„ë£Œ!')
    except Exception as e:
        print(f'âŒ ì»¬ë ‰ì…˜ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}')

<<<<<<< HEAD
=======
def get_llm(openai_key):
    return ChatOpenAI(api_key=openai_key, model="gpt-4o-mini", temperature=0.5)

def docs2tpl(results,answer_tpl,user_query,k=5):
    contexts = []
    full_documents = []
    for i, item in enumerate(results[:k]):
        contexts.append(f"{i+1}. {item['ìœ ì‚¬ë¬¸ë‹¨']} [íŒë¡€ë²ˆí˜¸:{item['íŒë¡€ì¼ë ¨ë²ˆí˜¸']}]")
        full_documents.append(f"--- ë¬¸ì„œ {i+1} ---\n{item['ì „ë¬¸']}")

    # kê°œì— ë§ê²Œ ë™ì ìœ¼ë¡œ ë Œë”ë§
    render_data = {'user_query': user_query}
    
    for i in range(k):
        if i < len(results):
            render_data[f'context{i+1}'] = results[i]['ìœ ì‚¬ë¬¸ë‹¨'] + f" [íŒë¡€ë²ˆí˜¸:{results[i]['íŒë¡€ì¼ë ¨ë²ˆí˜¸']}]"
            render_data[f'full{i+1}'] = results[i]['ì „ë¬¸']
        else:
            # kê°œë³´ë‹¤ ì ì€ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš° ë¹ˆ ë¬¸ìì—´ë¡œ ì±„ì›€
            render_data[f'context{i+1}'] = ""
            render_data[f'full{i+1}'] = ""
    
    answer = answer_tpl.render(**render_data)
    return answer

async def run_rag(user_query: str, vectorstore, k: int = 5, conn = None,answer_tpl = None,openai_key = None) -> str:
    # 1) ê²€ìƒ‰ì–´ ì¬ì‘ì„±
    #search_query = rewrite_query(user_query)
    #print("   â†ª ê²€ìƒ‰ì–´:", search_query)

    # 2) MySQL+Chroma í†µí•© ì „ë¬¸ ì¡°íšŒ (create_db.retrieve_db í˜¸ì¶œ)
    print("DB ê²€ìƒ‰ ì¤‘...")
    results = retrieve_db(
        user_query,
        conn,
        vectorstore,
        k=k
    )

    # 3) ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ ë° í…œí”Œë¦¿ ì ìš©
    if not results or len(results) == 0:
        return "ìœ ì‚¬ íŒë¡€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    # ìƒìœ„ kê°œ ê²°ê³¼ ëª¨ë‘ ë³‘í•©
    answer = docs2tpl(results,answer_tpl,user_query)

    llm = get_llm(openai_key)
    print("ğŸ”„ ë‹µë³€ ìƒì„±(LLM) â€¦", end=" ")
    start = time.perf_counter()
    
    # ìŠ¤íŠ¸ë¦¬ë°í•˜ë©´ì„œ ë‚´ìš©ì„ ëª¨ì•„ì„œ ë¦¬í„´
    full_response = ""
    async for chunk in llm.astream(answer):
        content = chunk.content
        if content:
            print(content, end="", flush=True)
            full_response += str(content)
    
    print(f"âœ” ({time.perf_counter()-start:.1f}s)")

    # ''' '''ë¡œ ê°ì‹¸ì§„ ë‹µë³€ì´ë©´ ì œê±°
    content = full_response.strip()
    if content.startswith("'''") and content.endswith("'''"):
        content = content[3:-3].strip()
    return content

async def run_rag_stream(user_query: str, vectorstore, k: int = 5, conn = None, answer_tpl = None, openai_key = None):
    """ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ - ê° ì²­í¬ë¥¼ yield"""
    # 1) ê²€ìƒ‰ì–´ ì¬ì‘ì„±
    print("DB ê²€ìƒ‰ ì¤‘...")
    results = retrieve_db(
        user_query,
        conn,
        vectorstore,
        k=k
    )

    # 3) ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ ë° í…œí”Œë¦¿ ì ìš©
    if not results or len(results) == 0:
        yield "ìœ ì‚¬ íŒë¡€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        return

    answer = docs2tpl(results,answer_tpl,user_query)

    llm = get_llm(openai_key)
    
    # ìŠ¤íŠ¸ë¦¬ë°í•˜ë©´ì„œ ê° ì²­í¬ë¥¼ yield
    async for chunk in llm.astream(answer):
        if not hasattr(run_rag_stream, '_first_chunk_printed'):
            print("ğŸ“Œ ë‹µë³€ \n", end="", flush=True)
            run_rag_stream._first_chunk_printed = True
        content = chunk.content
        if content:
            #print(content, end="", flush=True)
            yield str(content)

    # content ë³€ìˆ˜ê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ ì œê±°
    # async generatorì—ì„œëŠ” return ê°’ì„ ê°€ì§ˆ ìˆ˜ ì—†ìŒ

def set_conversation(query,answer,model):
    memory = ConversationBufferMemory()
    conversation = ConversationChain( 
        llm=model,
        memory=memory,
        verbose=True
    )
    memory.save_context({'input':query},{'output':answer})
    return conversation

async def run_conversation(conversation,user_query,vectorstore,conn,k=5,answer_tpl=None,openai_key=None):
    print("DB ê²€ìƒ‰ ì¤‘...")
    results = retrieve_db(
        user_query,
        conn,
        vectorstore,
        k=k,
        threshold=0.7
    )
    
    answer = docs2tpl(results,answer_tpl,user_query,k=3)
    
    async for chunk in conversation.astream(answer):
        # ConversationChain.astream() returns dictionaries with 'response' key
        if isinstance(chunk, dict) and 'response' in chunk:
            content = chunk['response']
            if content:
                yield str(content)

def setup_db(base_db_dir='./db'):
    cuda_available = torch.cuda.is_available()
    if cuda_available:
        print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
        print(f"   GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    else:
        print("âŒ CUDA ì‚¬ìš© ë¶ˆê°€ëŠ¥ - CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤")
    device = "cuda" if cuda_available else "cpu"
    #check_db()
    vectorstore = Chroma(
        persist_directory=base_db_dir,
        embedding_function=SentenceTransformerEmbeddings(model_name='nlpai-lab/KURE-v1', model_kwargs={"device": device}),
        collection_name='LAW_RAG_500_75'
    )
    print('ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ')
    return vectorstore

>>>>>>> 37c4389 (ë°°í¬ ì „ ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ)
if __name__ == "__main__":
    check_db()