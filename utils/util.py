from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings   
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.schema import Document
import pandas as pd
import pymysql
import os
import warnings
import chromadb
from langchain.embeddings import SentenceTransformerEmbeddings
import torch
import time
from datetime import datetime, timedelta
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder

warnings.filterwarnings('ignore')


def get_mysql_connection(host,port,username,password,db_name):
    try:
        conn = pymysql.connect(host=host, port=port, user=username, password=password, db=db_name,charset='utf8',cursorclass=pymysql.cursors.DictCursor)
        print(f"MySQL ì—°ê²° ì„±ê³µ")
        return conn
    except Exception as e:
        print(f"MySQL ì—°ê²° ì˜¤ë¥˜: {e}")
        return None

def get_document(conn,source):
    try: 
        with conn.cursor() as cursor:
            sql = f'SELECT íŒë¡€ì¼ë ¨ë²ˆí˜¸, íŒë¡€ë‚´ìš© FROM íŒë¡€ WHERE íŒë¡€ì¼ë ¨ë²ˆí˜¸ = {source}'
            cursor.execute(sql)
            result = cursor.fetchone()
            return result
    except Exception as e:
        print(f"MySQL ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return None


def multiquery_retrieve_db(query,host,port,username,password,db_name,base_db_dir='./db',k=1):
    cuda_available = torch.cuda.is_available()
    if cuda_available:
        print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
        print(f"   GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    else:
        print("âŒ CUDA ì‚¬ìš© ë¶ˆê°€ëŠ¥ - CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤")
    device = "cuda" if cuda_available else "cpu"
    
    vectorstore = Chroma(
        persist_directory=base_db_dir,
        embedding_function=SentenceTransformerEmbeddings(model_name='nlpai-lab/KURE-v1', model_kwargs={"device": device}),
        collection_name='LAW_RAG_500_75'
    )
    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": k, "fetch_k": 20, "lambda_mult": 0.5}
    )
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=1)
    prompt = PromptTemplate.from_template(
        """
        By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of distance-based similarity search.  
        Your response must be a list of values separated solely by new-line characters, e.g.  
        foo\nbar\nbaz\n

        #ORIGINAL QUESTION:  
        {question}

        #Answer in Korean:
        ### Role
        You are a â€œLegal-document RAGâ€ multi-query generator.  
        Based on the single incident scenario provided below, create **5** search queries that will surface a broad range of relevant case-law materials.

        ### Input format  
        [Incident scenario]

        ### Output rules  
        1. Output **one query per line**, max 120 characters (including spaces).  
        2. Provide only the raw query stringsâ€”no numbering, quotes, comments, or explanations.  
        3. Avoid duplicate or near-duplicate queries; use distinct wording and perspectives.  
        4. Separate lines with `\n` only, following the pattern `foo\nbar\nbaz\n`.

        ### Query-writing guidelines  
        - Restate the **key facts/acts** and **main legal issues** (tort, vicarious liability, bailment/custody, insurer subrogation, etc.) in varied ways.  
        - Restate the situation in a legal way so that it can be used as a search query.

        """
    )
    multiquery_retriever = MultiQueryRetriever.from_llm(
        retriever=retriever,
        llm=llm,
        prompt = prompt,
        include_original=True,
    )
    results = multiquery_retriever.invoke(query)
    conn = get_mysql_connection(host,port,username,password,db_name)
    # ê²°ê³¼ ì¶œë ¥
    output = []
    for i, doc in enumerate(results):
        meta = doc.metadata
        result = get_document(conn, meta['source'])
        output.append({
            "rank": i+1,
            "íŒë¡€ì¼ë ¨ë²ˆí˜¸": meta['source'],
            "ì‚¬ê±´ëª…": meta.get('case_type'),
            "ìœ ì‚¬ë¬¸ë‹¨": doc.page_content.strip(),
            "ì „ë¬¸": result['íŒë¡€ë‚´ìš©'] if result else None
        })
    return output

    
def retrieve_db(query,conn,vectorstore,k=1):
    
    retriever = vectorstore.as_retriever(
        search_type="mmr", 
        search_kwargs={"k": k, "fetch_k": 20, "lambda_mult": 0.85},
        return_metadata=True
    )
    
    '''cross_encoder = HuggingFaceCrossEncoder(model_name="BAAI/bge-reranker-v2-m3")
    compressor = CrossEncoderReranker(model=cross_encoder,top_n=k)
    compression_retriever = ContextualCompressionRetriever(
        base_compressor=compressor,
        base_retriever=retriever
    )
    
    results = compression_retriever.invoke(query)'''
    results = retriever.invoke(query)
    
    print('ë²¡í„°ìŠ¤í† ì–´ ê²€ìƒ‰ ì¤‘...')

    # ê²°ê³¼ ì¶œë ¥
    output = []
    for i, doc in enumerate(results):
        meta = doc.metadata
        result = get_document(conn, meta['source'])
        output.append({
        "rank": i+1,
        "íŒë¡€ì¼ë ¨ë²ˆí˜¸": meta['source'],
        "ì‚¬ê±´ëª…": meta.get('case_type'),
        "ìœ ì‚¬ë¬¸ë‹¨": doc.page_content.strip(),
        "ì „ë¬¸": result['íŒë¡€ë‚´ìš©'] if result else None
        })
        #print(meta['source'])
        #print(doc.page_content)
    return output


def check_db(base_db_dir='./db'):
    print(f'ğŸ” DB ê²½ë¡œ: {base_db_dir}')
    try:
        client = chromadb.PersistentClient(path=base_db_dir)
        collections = client.list_collections()
        if not collections:
            print('âŒ ì»¬ë ‰ì…˜ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.')
            return
        print(f'âœ… {len(collections)}ê°œì˜ ì»¬ë ‰ì…˜ì´ ì¡´ì¬í•©ë‹ˆë‹¤:')
        for col in collections:
            print(f'\nğŸ“ ì»¬ë ‰ì…˜ ì´ë¦„: {col.name}')
            print(f'  - id: {col.id}')
            print(f'  - ë©”íƒ€ë°ì´í„°: {col.metadata}')
            print(f'  - document count: {col.count()}')
    except Exception as e:
        print(f'âŒ DB í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}')
    
def delete_collection(collection_name, base_db_dir='./db'):
    print(f'ğŸ—‘ï¸ ì»¬ë ‰ì…˜ ì‚­ì œ ì‹œë„: {collection_name} (DB ê²½ë¡œ: {base_db_dir})')
    try:
        client = chromadb.PersistentClient(path=base_db_dir)
        client.delete_collection(name=collection_name)
        print(f'âœ… ì»¬ë ‰ì…˜ "{collection_name}" ì‚­ì œ ì™„ë£Œ!')
    except Exception as e:
        print(f'âŒ ì»¬ë ‰ì…˜ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}')

def get_llm(openai_key):
    return ChatOpenAI(api_key=openai_key, model="gpt-4o-mini", temperature=0.5)

def run_rag(user_query: str, vectorstore, k: int = 5, conn = None,answer_tpl = None,openai_key = None) -> str:
    # 1) ê²€ìƒ‰ì–´ ì¬ì‘ì„±
    #search_query = rewrite_query(user_query)
    #print("   â†ª ê²€ìƒ‰ì–´:", search_query)

    # 2) MySQL+Chroma í†µí•© ì „ë¬¸ ì¡°íšŒ (create_db.retrieve_db í˜¸ì¶œ)
    print("DB ê²€ìƒ‰ ì¤‘...")
    results = retrieve_db(
        user_query,
        conn,
        vectorstore,
        k=k
    )

    # 3) ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ ë° í…œí”Œë¦¿ ì ìš©
    if not results or len(results) == 0:
        return "ìœ ì‚¬ íŒë¡€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    # ìƒìœ„ kê°œ ê²°ê³¼ ëª¨ë‘ ë³‘í•©
    contexts = []
    full_documents = []

    for i, item in enumerate(results):
        # ê° ê²°ê³¼ì—ì„œ í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ
        contexts.append(f"{i+1}. {item['ìœ ì‚¬ë¬¸ë‹¨']} [íŒë¡€ë²ˆí˜¸:{item['íŒë¡€ì¼ë ¨ë²ˆí˜¸']}]")
        full_documents.append(f"--- ë¬¸ì„œ {i+1} ---\n{item['ì „ë¬¸']}")

    answer = answer_tpl.render(
        context1=results[0]['ìœ ì‚¬ë¬¸ë‹¨'] + f" [íŒë¡€ë²ˆí˜¸:{results[0]['íŒë¡€ì¼ë ¨ë²ˆí˜¸']}]",
        full1=results[0]['ì „ë¬¸'],
        context2=results[1]['ìœ ì‚¬ë¬¸ë‹¨'] + f" [íŒë¡€ë²ˆí˜¸:{results[1]['íŒë¡€ì¼ë ¨ë²ˆí˜¸']}]",
        full2=results[1]['ì „ë¬¸'],
        context3=results[2]['ìœ ì‚¬ë¬¸ë‹¨'] + f" [íŒë¡€ë²ˆí˜¸:{results[2]['íŒë¡€ì¼ë ¨ë²ˆí˜¸']}]",
        full3=results[2]['ì „ë¬¸'],
        context4=results[3]['ìœ ì‚¬ë¬¸ë‹¨'] + f" [íŒë¡€ë²ˆí˜¸:{results[3]['íŒë¡€ì¼ë ¨ë²ˆí˜¸']}]",
        full4=results[3]['ì „ë¬¸'],
        context5=results[4]['ìœ ì‚¬ë¬¸ë‹¨'] + f" [íŒë¡€ë²ˆí˜¸:{results[4]['íŒë¡€ì¼ë ¨ë²ˆí˜¸']}]",
        full5=results[4]['ì „ë¬¸'],
        user_query=user_query
    )

    llm = get_llm(openai_key)
    print("ğŸ”„ ë‹µë³€ ìƒì„±(LLM) â€¦", end=" ")
    start = time.perf_counter()
    resp = llm.invoke(answer)
    print(f"âœ” ({time.perf_counter()-start:.1f}s)")

    # ''' '''ë¡œ ê°ì‹¸ì§„ ë‹µë³€ì´ë©´ ì œê±°
    content = resp.content.strip() if hasattr(resp, "content") else resp.strip()
    if content.startswith("'''") and content.endswith("'''"):
        content = content[3:-3].strip()
    return content

def setup_db(base_db_dir='./db'):
    cuda_available = torch.cuda.is_available()
    if cuda_available:
        print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
        print(f"   GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    else:
        print("âŒ CUDA ì‚¬ìš© ë¶ˆê°€ëŠ¥ - CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤")
    device = "cuda" if cuda_available else "cpu"
    #check_db()
    vectorstore = Chroma(
        persist_directory=base_db_dir,
        embedding_function=SentenceTransformerEmbeddings(model_name='nlpai-lab/KURE-v1', model_kwargs={"device": device}),
        collection_name='LAW_RAG_500_75'
    )
    print('ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ')
    return vectorstore

if __name__ == "__main__":
    check_db()